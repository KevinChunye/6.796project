{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfb6e5eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Informer' from 'models.model' (/Users/haochuanwang/Documents/GitHub/6.796project/informer/models/model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Informer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Informer' from 'models.model' (/Users/haochuanwang/Documents/GitHub/6.796project/informer/models/model.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from models.model import Informer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the directory one level up (..)\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add it to the system path so Python can find files there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "# Import your synthetic data generation\n",
    "from synthetic_data import simulate_heavy_t_ar1, simulate_season_trend_outliers\n",
    "\n",
    "# Import the Informer model (ensure models/ folder is present)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Custom Dataset for Informer\n",
    "# -------------------------------------------------------------------\n",
    "class InformerDataset(Dataset):\n",
    "    def __init__(self, data, seq_len, label_len, pred_len, freq='h'):\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = label_len\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        # 1. Standardization\n",
    "        self.scaler = StandardScaler()\n",
    "        self.data_x = self.scaler.fit_transform(data)\n",
    "        self.data_y = self.data_x \n",
    "\n",
    "        # 2. Create Dummy Timestamps\n",
    "        dates = pd.date_range(start='2020-01-01', periods=len(data), freq=freq)\n",
    "        \n",
    "        # FIX: Use .dt accessor instead of .apply()\n",
    "        df_stamp = pd.DataFrame({'date': dates})\n",
    "        df_stamp['month'] = df_stamp['date'].dt.month\n",
    "        df_stamp['day'] = df_stamp['date'].dt.day\n",
    "        df_stamp['weekday'] = df_stamp['date'].dt.dayofweek\n",
    "        df_stamp['hour'] = df_stamp['date'].dt.hour\n",
    "        \n",
    "        self.data_stamp = df_stamp.drop(columns=['date']).values\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "\n",
    "        dec_x = self.data_x[r_begin:r_end]\n",
    "        dec_x[self.label_len:, :] = 0 \n",
    "        dec_x_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, dec_x, dec_x_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "    \n",
    "# -------------------------------------------------------------------\n",
    "# 2. The Training Loop (Adapted for Informer Inputs)\n",
    "# -------------------------------------------------------------------\n",
    "def train_informer_on_series(\n",
    "    series,\n",
    "    seq_len=96,\n",
    "    label_len=48,\n",
    "    pred_len=24,\n",
    "    batch_size=32,\n",
    "    n_epochs=5,\n",
    "    lr=1e-4,\n",
    "    device=None\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Ensure Series shape (T, 1)\n",
    "    if len(series.shape) == 1:\n",
    "        series = series.reshape(-1, 1)\n",
    "\n",
    "    # Prepare Data\n",
    "    dataset = InformerDataset(series, seq_len, label_len, pred_len)\n",
    "    \n",
    "    # Split Train/Val (Simple split for demo)\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    # Initialize Informer\n",
    "    # enc_in=1, dec_in=1, c_out=1 because our synthetic data is univariate\n",
    "    model = Informer(\n",
    "        enc_in=1, dec_in=1, c_out=1, \n",
    "        seq_len=seq_len, label_len=label_len, out_len=pred_len,\n",
    "        factor=5, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, \n",
    "        dropout=0.05, attn='prob', embed='fixed', freq='h', activation='gelu', \n",
    "        output_attention=False, distil=True, mix=True,\n",
    "        device=device\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    print(f\"Start Training on {device}...\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        \n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_dec_x, batch_dec_x_mark) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Move to device & cast\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_dec_x = batch_dec_x.float().to(device)\n",
    "            batch_dec_x_mark = batch_dec_x_mark.float().to(device)\n",
    "\n",
    "            # Informer Forward Pass\n",
    "            # Enc_out is not used here, we only need the decoder output for loss\n",
    "            outputs = model(batch_x, batch_x_mark, batch_dec_x, batch_dec_x_mark)\n",
    "\n",
    "            # Informer outputs [Batch, Pred_Len, Features]\n",
    "            # We crop the target 'batch_y' to match the prediction length (last pred_len steps)\n",
    "            f_dim = -1 if False else 0 # 0 for univariate\n",
    "            outputs = outputs[:, -pred_len:, f_dim:]\n",
    "            batch_y = batch_y[:, -pred_len:, f_dim:].to(device)\n",
    "\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_dec_x, batch_dec_x_mark) in enumerate(val_loader):\n",
    "                batch_x = batch_x.float().to(device)\n",
    "                batch_y = batch_y.float().to(device)\n",
    "                batch_x_mark = batch_x_mark.float().to(device)\n",
    "                batch_dec_x = batch_dec_x.float().to(device)\n",
    "                batch_dec_x_mark = batch_dec_x_mark.float().to(device)\n",
    "\n",
    "                outputs = model(batch_x, batch_x_mark, batch_dec_x, batch_dec_x_mark)\n",
    "                \n",
    "                outputs = outputs[:, -pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -pred_len:, f_dim:]\n",
    "                \n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss {np.average(train_loss):.5f} | Val Loss {np.average(val_loss):.5f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Main Execution\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # A. Generate Data (Using your synthetic_data.py function)\n",
    "    print(\"Generating Synthetic Data...\")\n",
    "    series, _ = simulate_season_trend_outliers(\n",
    "        T=4000, \n",
    "        season_period=48, # e.g. 2-day seasonality if hourly\n",
    "        n_outliers=10, \n",
    "        outlier_magnitude=5.0\n",
    "    )\n",
    "    \n",
    "    # B. Train Informer\n",
    "    model = train_informer_on_series(\n",
    "        series, \n",
    "        seq_len=96,   # Look back 96 steps\n",
    "        label_len=48, # Known history provided to decoder\n",
    "        pred_len=24,  # Predict next 24 steps\n",
    "        n_epochs=3\n",
    "    )\n",
    "    \n",
    "    print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582dbc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
