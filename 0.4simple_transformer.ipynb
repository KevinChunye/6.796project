{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 0. Core model + dataset (keep as-is)\n",
    "# -------- Sliding-window dataset --------\n",
    "\n",
    "class SlidingWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Turn a time series (T, N) into input/target windows for forecasting.\n",
    "\n",
    "    series: np.ndarray, shape (T,) or (T, N)\n",
    "    input_len: length of history window\n",
    "    pred_len:  length of prediction horizon\n",
    "    \"\"\"\n",
    "    def __init__(self, series, input_len, pred_len, stride=1):\n",
    "        if series.ndim == 1:\n",
    "            series = series[:, None]  # (T,) -> (T,1)\n",
    "        T, N = series.shape\n",
    "\n",
    "        X_list, y_list = [], []\n",
    "        for start in range(0, T - input_len - pred_len + 1, stride):\n",
    "            end = start + input_len\n",
    "            target_end = end + pred_len\n",
    "            X_list.append(series[start:end])        # (input_len, N)\n",
    "            y_list.append(series[end:target_end])   # (pred_len, N)\n",
    "\n",
    "        self.X = torch.from_numpy(np.stack(X_list)).float()  # (B, L, N)\n",
    "        self.y = torch.from_numpy(np.stack(y_list)).float()  # (B, H, N)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# -------- Positional encodings --------\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"Standard Transformer sinusoidal positional encoding.\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n",
    "\n",
    "\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    \"\"\"Learnable positional embeddings (one vector per index).\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq_len, _ = x.size()\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(b, -1)\n",
    "        return x + self.pos_embed(positions)\n",
    "\n",
    "\n",
    "# -------- Time-series Transformer --------\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple encoder-only Transformer for time-series forecasting.\n",
    "\n",
    "    - Supports sinusoidal or learnable positional encoding.\n",
    "    - Predicts 'pred_len' future steps for each input series.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=1,\n",
    "        d_model=64,\n",
    "        n_heads=4,\n",
    "        num_layers=2,\n",
    "        dim_feedforward=128,\n",
    "        dropout=0.1,\n",
    "        pred_len=1,\n",
    "        pos_encoding_type=\"sin\",  # \"sin\" or \"learned\"\n",
    "        max_len=500,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        if pos_encoding_type == \"sin\":\n",
    "            self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "        elif pos_encoding_type == \"learned\":\n",
    "            self.pos_encoding = LearnablePositionalEncoding(d_model, max_len)\n",
    "        else:\n",
    "            self.pos_encoding = None  # no PE (for ablation)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,   # (B, L, D)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, pred_len * input_dim)\n",
    "        self.attn_weights = []  # Store attention weights\n",
    "\n",
    "\n",
    "        def custom_forward(module, src, src_mask=None, src_key_padding_mask=None):\n",
    "            attn_output, attn_weight = module.self_attn(\n",
    "                src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask, need_weights=True, average_attn_weights=False\n",
    "            )\n",
    "            module.attn_weight = attn_weight  # save for extraction\n",
    "            src = module.norm1(src + module.dropout1(attn_output))\n",
    "            src = module.norm2(src + module.dropout2(module.linear2(module.dropout(module.activation(module.linear1(src))))))\n",
    "            return src\n",
    "\n",
    "        for layer in self.encoder.layers:\n",
    "            layer.forward = lambda src, layer=layer: custom_forward(layer, src)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_dim)\n",
    "        returns: (batch, pred_len, input_dim)\n",
    "        \"\"\"\n",
    "        h = self.input_proj(x)  # (B, L, d_model)\n",
    "\n",
    "        if self.pos_encoding is not None:\n",
    "            h = self.pos_encoding(h)\n",
    "\n",
    "        h = self.encoder(h)     # (B, L, d_model)\n",
    "        h_last = h[:, -1, :]    # (B, d_model)\n",
    "\n",
    "        out = self.fc_out(h_last)                     # (B, pred_len * input_dim)\n",
    "        out = out.view(-1, self.pred_len, self.input_dim)  # (B, pred_len, input_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def plot_attention_map(attn, input_len, title=\"\", head=0):\n",
    "    \"\"\"\n",
    "    attn: tensor of shape (num_heads, B, T_q, T_k)\n",
    "    \"\"\"\n",
    "    head_attn = attn[head, 0]  # choose head 0, batch 0\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(head_attn[:, :input_len], cmap=\"viridis\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Key Time Step\")\n",
    "    plt.ylabel(\"Query Time Step\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7743aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Single training wrapper (reuse everywhere)\n",
    "def train_transformer_on_series(\n",
    "    series,\n",
    "    input_len=64,\n",
    "    pred_len=1,\n",
    "    batch_size=32,\n",
    "    n_epochs=10,\n",
    "    lr=1e-3,\n",
    "    pos_encoding_type=\"sin\",\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a TimeSeriesTransformer on one time series.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : nn.Module\n",
    "    history : dict with keys 'train_loss', 'val_loss'\n",
    "    test_loss : float (MSE on held-out test set)\n",
    "    splits : (train_set, val_set, test_set)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Ensure shape (T, N)\n",
    "    if isinstance(series, np.ndarray) and series.ndim == 1:\n",
    "        series = series[:, None]\n",
    "    elif torch.is_tensor(series) and series.ndim == 1:\n",
    "        series = series.unsqueeze(-1).cpu().numpy()\n",
    "\n",
    "    dataset = SlidingWindowDataset(series, input_len, pred_len)\n",
    "    n_total = len(dataset)\n",
    "    n_train = int(0.7 * n_total)\n",
    "    n_val = int(0.15 * n_total)\n",
    "    n_test = n_total - n_train - n_val\n",
    "\n",
    "    train_set, val_set, test_set = torch.utils.data.random_split(\n",
    "        dataset,\n",
    "        [n_train, n_val, n_test],\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_set,   batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_set,  batch_size=batch_size)\n",
    "\n",
    "    model = TimeSeriesTransformer(\n",
    "        input_dim=dataset.X.shape[-1],\n",
    "        d_model=32,\n",
    "        n_heads=4,\n",
    "        num_layers=1,\n",
    "        dim_feedforward=64,\n",
    "        dropout=0.1,\n",
    "        pred_len=pred_len,\n",
    "        pos_encoding_type=pos_encoding_type,\n",
    "        max_len=input_len,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # --- validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{n_epochs} \"\n",
    "            f\"| train={train_loss:.4f} | val={val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    # --- test ---\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test MSE: {test_loss:.4f}\")\n",
    "\n",
    "    return model, history, test_loss, (train_set, val_set, test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d00db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_suite(\n",
    "    series_dict,\n",
    "    mode_label,\n",
    "    horizons=(1, 10),\n",
    "    n_epochs=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Transformer on all families, all pos_enc variants, and all horizons.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series_dict : dict\n",
    "        Maps family name to config:\n",
    "            {\n",
    "                \"series\": np.array (T,),\n",
    "                \"input_len\": int,\n",
    "                \"pos_encs\": [\"sin\", \"learned\", ...],\n",
    "            }\n",
    "    mode_label : str\n",
    "        E.g., \"raw\" or \"stationary\"\n",
    "    horizons : tuple of int\n",
    "        Forecast steps to evaluate.\n",
    "    n_epochs : int\n",
    "        Training epochs for each run.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_df : pd.DataFrame\n",
    "        One row per (family, pos_enc, horizon).\n",
    "    histories : dict\n",
    "        Keys = (mode, family, pos_enc, horizon) → training history.\n",
    "    models : dict\n",
    "        Keys = same as above → trained model.\n",
    "    attn_outputs : dict\n",
    "        Keys = same as above → list of attention maps (one per layer).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    histories = {}\n",
    "    models = {}\n",
    "    attn_outputs = {}\n",
    "\n",
    "    for family, cfg in series_dict.items():\n",
    "        series = cfg[\"series\"]\n",
    "        input_len = cfg[\"input_len\"]\n",
    "        pos_encs = cfg[\"pos_encs\"]\n",
    "\n",
    "        for horizon in horizons:\n",
    "            for pos_enc in pos_encs:\n",
    "                print(f\"\\n=== {mode_label} | {family} | pos_enc={pos_enc} | horizon={horizon} ===\")\n",
    "                model, history, test_loss, splits = train_transformer_on_series(\n",
    "                    series,\n",
    "                    input_len=input_len,\n",
    "                    pred_len=horizon,\n",
    "                    n_epochs=n_epochs,\n",
    "                    pos_encoding_type=pos_enc,\n",
    "                )\n",
    "\n",
    "                key = (mode_label, family, pos_enc, horizon)\n",
    "                histories[key] = history\n",
    "                models[key] = model\n",
    "\n",
    "                # Extract one attention map from test set\n",
    "                with torch.no_grad():\n",
    "                    test_loader = DataLoader(splits[2], batch_size=1)\n",
    "                    X_sample, _ = next(iter(test_loader))\n",
    "                    model.eval()\n",
    "                    _ = model(X_sample)  # forward to trigger .attn_weights\n",
    "                    attn_outputs[key] = model.attn_weights  # list of (H, B, L, L)\n",
    "\n",
    "                results.append({\n",
    "                    \"mode\": mode_label,\n",
    "                    \"family\": family,\n",
    "                    \"pos_enc\": pos_enc,\n",
    "                    \"horizon\": horizon,\n",
    "                    \"test_MSE\": float(test_loss),\n",
    "                })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df, histories, models, attn_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51dd24c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'simulate_trend_breaks' from 'synthetic_data' (/Users/haochuanwang/Documents/GitHub/6.796project/synthetic_data.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msynthetic_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     simulate_heavy_t_ar1,\n\u001b[1;32m      3\u001b[0m     simulate_garch_11,\n\u001b[1;32m      4\u001b[0m     simulate_regime_switching_mean,\n\u001b[1;32m      5\u001b[0m     simulate_1_over_f_noise,\n\u001b[1;32m      6\u001b[0m     simulate_season_trend_outliers,\n\u001b[1;32m      7\u001b[0m     simulate_random_walk,\n\u001b[1;32m      8\u001b[0m     simulate_jump_diffusion,\n\u001b[1;32m      9\u001b[0m     simulate_multi_seasonality,\n\u001b[1;32m     10\u001b[0m     simulate_trend_breaks,  \n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzscore\u001b[39m(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m):\n\u001b[1;32m     14\u001b[0m     mean \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'simulate_trend_breaks' from 'synthetic_data' (/Users/haochuanwang/Documents/GitHub/6.796project/synthetic_data.py)"
     ]
    }
   ],
   "source": [
    "from synthetic_data import (\n",
    "    simulate_heavy_t_ar1,\n",
    "    simulate_garch_11,\n",
    "    simulate_regime_switching_mean,\n",
    "    simulate_1_over_f_noise,\n",
    "    simulate_season_trend_outliers,\n",
    "    simulate_random_walk,\n",
    "    simulate_jump_diffusion,\n",
    "    simulate_multi_seasonality,\n",
    "    simulate_trend_breaks,  \n",
    ")\n",
    "\n",
    "def zscore(X, axis=0, eps=1e-8):\n",
    "    mean = X.mean(axis=axis, keepdims=True)\n",
    "    std = X.std(axis=axis, keepdims=True)\n",
    "    return (X - mean) / (std + eps)\n",
    "\n",
    "# Custom normalization based on each problem:\n",
    "def make_stationary_ar1(X):  # heavy tail\n",
    "    return zscore(X, axis=0)\n",
    "\n",
    "def make_stationary_garch(r):  # time-varying volatility\n",
    "    return zscore(r, axis=0)\n",
    "\n",
    "def make_stationary_regime(X):  # regime shifts\n",
    "    return zscore(np.diff(X, axis=0), axis=0)\n",
    "\n",
    "def make_stationary_long_memory(X):\n",
    "    return zscore(X, axis=0)\n",
    "\n",
    "def make_stationary_season_trend(Y, season_period):\n",
    "    Y1 = np.diff(Y, axis=0)\n",
    "    Y2 = Y1[season_period:] - Y1[:-season_period]\n",
    "    return zscore(Y2, axis=0)\n",
    "\n",
    "def make_stationary_random_walk(X):  # non-stationary mean\n",
    "    return zscore(np.diff(X, axis=0), axis=0)\n",
    "\n",
    "def make_stationary_jump_diffusion(X):  # outliers\n",
    "    X = np.clip(X, np.percentile(X, 1), np.percentile(X, 99))  # mild outlier removal\n",
    "    return zscore(X, axis=0)\n",
    "\n",
    "def make_stationary_multi_season(X, period1, period2):\n",
    "    X1 = X[period1:] - X[:-period1]\n",
    "    X2 = X1[period2:] - X1[:-period2]\n",
    "    return zscore(X2, axis=0)\n",
    "\n",
    "def make_stationary_trend_break(X):\n",
    "    return zscore(np.diff(X, axis=0), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bcd2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6acad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16932b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
