{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 0. Core model + dataset (keep as-is)\n",
    "# -------- Sliding-window dataset --------\n",
    "\n",
    "class SlidingWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Turn a time series (T, N) into input/target windows for forecasting.\n",
    "\n",
    "    series: np.ndarray, shape (T,) or (T, N)\n",
    "    input_len: length of history window\n",
    "    pred_len:  length of prediction horizon\n",
    "    \"\"\"\n",
    "    def __init__(self, series, input_len, pred_len, stride=1):\n",
    "        if series.ndim == 1:\n",
    "            series = series[:, None]  # (T,) -> (T,1)\n",
    "        T, N = series.shape\n",
    "\n",
    "        X_list, y_list = [], []\n",
    "        for start in range(0, T - input_len - pred_len + 1, stride):\n",
    "            end = start + input_len\n",
    "            target_end = end + pred_len\n",
    "            X_list.append(series[start:end])        # (input_len, N)\n",
    "            y_list.append(series[end:target_end])   # (pred_len, N)\n",
    "\n",
    "        self.X = torch.from_numpy(np.stack(X_list)).float()  # (B, L, N)\n",
    "        self.y = torch.from_numpy(np.stack(y_list)).float()  # (B, H, N)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# -------- Positional encodings --------\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"Standard Transformer sinusoidal positional encoding.\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n",
    "\n",
    "\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    \"\"\"Learnable positional embeddings (one vector per index).\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq_len, _ = x.size()\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(b, -1)\n",
    "        return x + self.pos_embed(positions)\n",
    "\n",
    "\n",
    "# -------- Time-series Transformer --------\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple encoder-only Transformer for time-series forecasting.\n",
    "\n",
    "    - Supports sinusoidal or learnable positional encoding.\n",
    "    - Predicts 'pred_len' future steps for each input series.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=1,\n",
    "        d_model=64,\n",
    "        n_heads=4,\n",
    "        num_layers=2,\n",
    "        dim_feedforward=128,\n",
    "        dropout=0.1,\n",
    "        pred_len=1,\n",
    "        pos_encoding_type=\"sin\",  # \"sin\" or \"learned\"\n",
    "        max_len=500,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        if pos_encoding_type == \"sin\":\n",
    "            self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "        elif pos_encoding_type == \"learned\":\n",
    "            self.pos_encoding = LearnablePositionalEncoding(d_model, max_len)\n",
    "        else:\n",
    "            self.pos_encoding = None  # no PE (for ablation)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,   # (B, L, D)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, pred_len * input_dim)\n",
    "        self.attn_weights = []  # Store attention weights\n",
    "\n",
    "\n",
    "        def custom_forward(module, src, src_mask=None, src_key_padding_mask=None):\n",
    "            attn_output, attn_weight = module.self_attn(\n",
    "                src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask, need_weights=True, average_attn_weights=False\n",
    "            )\n",
    "            module.attn_weight = attn_weight  # save for extraction\n",
    "            src = module.norm1(src + module.dropout1(attn_output))\n",
    "            src = module.norm2(src + module.dropout2(module.linear2(module.dropout(module.activation(module.linear1(src))))))\n",
    "            return src\n",
    "\n",
    "    \n",
    "        for layer in self.encoder.layers:\n",
    "            def layer_forward(src, *args, **kwargs):\n",
    "                return custom_forward(layer, src, src_mask=kwargs.get(\"src_mask\"), src_key_padding_mask=kwargs.get(\"src_key_padding_mask\"))\n",
    "            layer.forward = layer_forward\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_dim)\n",
    "        returns: (batch, pred_len, input_dim)\n",
    "        \"\"\"\n",
    "        h = self.input_proj(x)  # (B, L, d_model)\n",
    "\n",
    "        if self.pos_encoding is not None:\n",
    "            h = self.pos_encoding(h)\n",
    "\n",
    "        h = self.encoder(h)     # (B, L, d_model)\n",
    "        h_last = h[:, -1, :]    # (B, d_model)\n",
    "\n",
    "        out = self.fc_out(h_last)                     # (B, pred_len * input_dim)\n",
    "        out = out.view(-1, self.pred_len, self.input_dim)  # (B, pred_len, input_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def plot_attention_map(attn, input_len, title=\"\", head=0):\n",
    "    \"\"\"\n",
    "    attn: tensor of shape (num_heads, B, T_q, T_k)\n",
    "    \"\"\"\n",
    "    head_attn = attn[head, 0]  # choose head 0, batch 0\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(head_attn[:, :input_len], cmap=\"viridis\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Key Time Step\")\n",
    "    plt.ylabel(\"Query Time Step\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7743aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Single training wrapper (reuse everywhere)\n",
    "def train_transformer_on_series(\n",
    "    series,\n",
    "    input_len=64,\n",
    "    pred_len=1,\n",
    "    batch_size=32,\n",
    "    n_epochs=10,\n",
    "    lr=1e-3,\n",
    "    pos_encoding_type=\"sin\",\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a TimeSeriesTransformer on one time series.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : nn.Module\n",
    "    history : dict with keys 'train_loss', 'val_loss'\n",
    "    test_loss : float (MSE on held-out test set)\n",
    "    splits : (train_set, val_set, test_set)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Ensure shape (T, N)\n",
    "    if isinstance(series, np.ndarray) and series.ndim == 1:\n",
    "        series = series[:, None]\n",
    "    elif torch.is_tensor(series) and series.ndim == 1:\n",
    "        series = series.unsqueeze(-1).cpu().numpy()\n",
    "\n",
    "    dataset = SlidingWindowDataset(series, input_len, pred_len)\n",
    "    n_total = len(dataset)\n",
    "    n_train = int(0.7 * n_total)\n",
    "    n_val = int(0.15 * n_total)\n",
    "    n_test = n_total - n_train - n_val\n",
    "\n",
    "    train_set, val_set, test_set = torch.utils.data.random_split(\n",
    "        dataset,\n",
    "        [n_train, n_val, n_test],\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_set,   batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_set,  batch_size=batch_size)\n",
    "\n",
    "    model = TimeSeriesTransformer(\n",
    "        input_dim=dataset.X.shape[-1],\n",
    "        d_model=32,\n",
    "        n_heads=4,\n",
    "        num_layers=1,\n",
    "        dim_feedforward=64,\n",
    "        dropout=0.1,\n",
    "        pred_len=pred_len,\n",
    "        pos_encoding_type=pos_encoding_type,\n",
    "        max_len=input_len,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # --- validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{n_epochs} \"\n",
    "            f\"| train={train_loss:.4f} | val={val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    # --- test ---\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test MSE: {test_loss:.4f}\")\n",
    "\n",
    "    return model, history, test_loss, (train_set, val_set, test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1d00db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_suite(\n",
    "    series_dict,\n",
    "    mode_label,\n",
    "    horizons=(1, 10),\n",
    "    n_epochs=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Transformer on all families, all pos_enc variants, and all horizons.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series_dict : dict\n",
    "        Maps family name to config:\n",
    "            {\n",
    "                \"series\": np.array (T,),\n",
    "                \"input_len\": int,\n",
    "                \"pos_encs\": [\"sin\", \"learned\", ...],\n",
    "            }\n",
    "    mode_label : str\n",
    "        E.g., \"raw\" or \"stationary\"\n",
    "    horizons : tuple of int\n",
    "        Forecast steps to evaluate.\n",
    "    n_epochs : int\n",
    "        Training epochs for each run.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_df : pd.DataFrame\n",
    "        One row per (family, pos_enc, horizon).\n",
    "    histories : dict\n",
    "        Keys = (mode, family, pos_enc, horizon) → training history.\n",
    "    models : dict\n",
    "        Keys = same as above → trained model.\n",
    "    attn_outputs : dict\n",
    "        Keys = same as above → list of attention maps (one per layer).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    histories = {}\n",
    "    models = {}\n",
    "    attn_outputs = {}\n",
    "\n",
    "    for family, cfg in series_dict.items():\n",
    "        series = cfg[\"series\"]\n",
    "        input_len = cfg[\"input_len\"]\n",
    "        pos_encs = cfg[\"pos_encs\"]\n",
    "\n",
    "        for horizon in horizons:\n",
    "            for pos_enc in pos_encs:\n",
    "                print(f\"\\n=== {mode_label} | {family} | pos_enc={pos_enc} | horizon={horizon} ===\")\n",
    "                model, history, test_loss, splits = train_transformer_on_series(\n",
    "                    series,\n",
    "                    input_len=input_len,\n",
    "                    pred_len=horizon,\n",
    "                    n_epochs=n_epochs,\n",
    "                    pos_encoding_type=pos_enc,\n",
    "                )\n",
    "\n",
    "                key = (mode_label, family, pos_enc, horizon)\n",
    "                histories[key] = history\n",
    "                models[key] = model\n",
    "\n",
    "                # Extract one attention map from test set\n",
    "                with torch.no_grad():\n",
    "                    test_loader = DataLoader(splits[2], batch_size=1)\n",
    "                    X_sample, _ = next(iter(test_loader))\n",
    "                    model.eval()\n",
    "                    _ = model(X_sample)  # forward to trigger .attn_weights\n",
    "                    attn_outputs[key] = model.attn_weights  # list of (H, B, L, L)\n",
    "\n",
    "                results.append({\n",
    "                    \"mode\": mode_label,\n",
    "                    \"family\": family,\n",
    "                    \"pos_enc\": pos_enc,\n",
    "                    \"horizon\": horizon,\n",
    "                    \"test_MSE\": float(test_loss),\n",
    "                })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df, histories, models, attn_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51dd24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_data import (\n",
    "    simulate_heavy_t_ar1,\n",
    "    simulate_garch_11,\n",
    "    simulate_regime_switching_mean,\n",
    "    simulate_1_over_f_noise,\n",
    "    simulate_season_trend_outliers,\n",
    "    simulate_random_walk,\n",
    "    simulate_jump_diffusion,\n",
    "    simulate_multi_seasonality,\n",
    "    simulate_trend_breaks,  \n",
    ")\n",
    "\n",
    "\n",
    "def zscore(X, axis=0, eps=1e-8):\n",
    "    mean = X.mean(axis=axis, keepdims=True)\n",
    "    std = X.std(axis=axis, keepdims=True)\n",
    "    return (X - mean) / (std + eps)\n",
    "\n",
    "# Fix functions\n",
    "def make_stationary_ar1(X): return zscore(X)\n",
    "def make_stationary_garch(r): return zscore(r)\n",
    "def make_stationary_regime(X): return zscore(np.diff(X, axis=0))\n",
    "def make_stationary_long_memory(X): return zscore(X)\n",
    "def make_stationary_season_trend(Y, season_period):\n",
    "    dY = np.diff(Y, axis=0)\n",
    "    if dY.shape[0] <= season_period:\n",
    "        raise ValueError(\"Too short for season differencing\")\n",
    "    return zscore(dY[season_period:] - dY[:-season_period])\n",
    "def make_stationary_random_walk(X): return zscore(np.diff(X, axis=0))\n",
    "def make_stationary_jump_diffusion(X):\n",
    "    X = np.clip(X, np.percentile(X, 1), np.percentile(X, 99))\n",
    "    return zscore(X)\n",
    "def make_stationary_multi_season(X, period1, period2):\n",
    "    X1 = X[period1:] - X[:-period1]\n",
    "    X2 = X1[period2:] - X1[:-period2]\n",
    "    return zscore(X2)\n",
    "def make_stationary_trend_break(X): return zscore(np.diff(X, axis=0))\n",
    "\n",
    "\n",
    "def fix_distribution(family, series):\n",
    "    if family == \"Heavy-tailed AR(1)\":\n",
    "        return zscore(series)\n",
    "    elif family == \"GARCH(1,1)\":\n",
    "        return zscore(series)\n",
    "    elif family == \"Regime-switching\":\n",
    "        return zscore(np.diff(series, axis=0))\n",
    "    elif family == \"1/f noise\":\n",
    "        return zscore(series)\n",
    "    elif family == \"Season+Trend+Outliers\":\n",
    "        dY = np.diff(series, axis=0)\n",
    "        if dY.shape[0] > 50:\n",
    "            season_adj = dY[50:] - dY[:-50]\n",
    "            return zscore(season_adj)\n",
    "        else:\n",
    "            return zscore(dY)\n",
    "    elif family in [\"Random Walk\", \"Jump Diffusion\", \"Trend Breaks\"]:\n",
    "        return zscore(np.diff(series, axis=0))\n",
    "    elif family == \"Multi-Seasonality\":\n",
    "        return zscore(series)\n",
    "    else:\n",
    "        raise ValueError(f\"No fix defined for: {family}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57bcd2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Synthetic generation\n",
    "T = 5000\n",
    "n_epochs = 10\n",
    "X_A = simulate_heavy_t_ar1(T, N=1, seed=0)\n",
    "r_B, _ = simulate_garch_11(T, N=1, seed=0)\n",
    "X_C, _ = simulate_regime_switching_mean(T, N=1, seed=0)\n",
    "X_D = simulate_1_over_f_noise(T, N=1, seed=0)\n",
    "Y_E, _ = simulate_season_trend_outliers(T=2000, N=1, seed=0)\n",
    "RW = simulate_random_walk(T, N=1, seed=0)\n",
    "JD = simulate_jump_diffusion(T, N=1, seed=0)\n",
    "MS = simulate_multi_seasonality(T, N=1, freqs=(50, 150), amps=(1.0, 0.8), seed=0)\n",
    "TB = simulate_trend_breaks(T, N=1, seed=0)\n",
    "\n",
    "# Raw dictionary\n",
    "raw_dict = {\n",
    "    \"Heavy-tailed AR(1)\": {\"series\": X_A[:, 0], \"input_len\": 64, \"pos_encs\": [\"sin\", \"learned\"]},\n",
    "    \"GARCH(1,1)\":         {\"series\": r_B[:, 0], \"input_len\": 64, \"pos_encs\": [\"sin\"]},\n",
    "    \"Regime-switching\":   {\"series\": X_C[:, 0], \"input_len\": 64, \"pos_encs\": [\"sin\"]},\n",
    "    \"1/f noise\":          {\"series\": X_D[:, 0], \"input_len\": 128, \"pos_encs\": [\"sin\"]},\n",
    "    \"Season+Trend+Outliers\": {\"series\": Y_E[:, 0], \"input_len\": 128, \"pos_encs\": [\"sin\"]},\n",
    "    \"Random Walk\":        {\"series\": RW[:, 0], \"input_len\": 128, \"pos_encs\": [\"sin\"]},\n",
    "    \"Jump Diffusion\":     {\"series\": JD[:, 0], \"input_len\": 128, \"pos_encs\": [\"sin\"]},\n",
    "    \"Multi-Seasonality\":  {\"series\": MS[:, 0], \"input_len\": 128, \"pos_encs\": [\"sin\"]},\n",
    "    \"Trend Breaks\":       {\"series\": TB[:, 0], \"input_len\": 128, \"pos_encs\": [\"sin\"]},\n",
    "}\n",
    "\n",
    "# Stationary versions\n",
    "stationary_dict = {\n",
    "    \"Heavy-tailed AR(1)\": (X_A[:, 0], make_stationary_ar1(X_A)[:, 0], 64),\n",
    "    \"GARCH(1,1)\":         (r_B[:, 0], make_stationary_garch(r_B)[:, 0], 64),\n",
    "    \"Regime-switching\":   (X_C[:, 0], make_stationary_regime(X_C)[:, 0], 64),\n",
    "    \"1/f noise\":          (X_D[:, 0], make_stationary_long_memory(X_D)[:, 0], 128),\n",
    "    \"Season+Trend+Outliers\": (Y_E[:, 0], make_stationary_season_trend(Y_E, 50)[:, 0], 128),\n",
    "    \"Random Walk\":        (RW[:, 0], make_stationary_random_walk(RW)[:, 0], 128),\n",
    "    \"Jump Diffusion\":     (JD[:, 0], make_stationary_jump_diffusion(JD)[:, 0], 128),\n",
    "    \"Multi-Seasonality\":  (MS[:, 0], make_stationary_multi_season(MS, 50, 150)[:, 0], 128),\n",
    "    \"Trend Breaks\":       (TB[:, 0], make_stationary_trend_break(TB)[:, 0], 128),\n",
    "}\n",
    "\n",
    "# Convert to fixed_dict format\n",
    "fixed_dict = {\n",
    "    name: {\n",
    "        \"series\": fixed,\n",
    "        \"input_len\": input_len,\n",
    "        \"pos_encs\": raw_dict[name][\"pos_encs\"],\n",
    "    }\n",
    "    for name, (_, fixed, input_len) in stationary_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51a6acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_suite(\n",
    "    series_dict,\n",
    "    mode_label,\n",
    "    horizons=(1, 10),\n",
    "    n_epochs=10,\n",
    "    preprocessing_fn=None,  # Optional preprocessing applied to each series\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Transformer on all families, all pos_enc variants, and all horizons.\n",
    "    \n",
    "    series_dict[family] = {\n",
    "        \"series\": np.array (T,),\n",
    "        \"input_len\": int,\n",
    "        \"pos_encs\": [\"sin\", \"learned\", ...],\n",
    "    }\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    histories = {}\n",
    "    models = {}\n",
    "\n",
    "    for family, cfg in series_dict.items():\n",
    "        series = cfg[\"series\"]\n",
    "        input_len = cfg[\"input_len\"]\n",
    "        pos_encs = cfg[\"pos_encs\"]\n",
    "\n",
    "        # Apply fix or transformation if provided\n",
    "        if preprocessing_fn is not None:\n",
    "            try:\n",
    "                series = preprocessing_fn(family, series)\n",
    "            except Exception as e:\n",
    "                print(f\"[Warning] Preprocessing failed for {family}: {e}\")\n",
    "                continue\n",
    "\n",
    "        for horizon in horizons:\n",
    "            for pos_enc in pos_encs:\n",
    "                print(f\"\\n=== {mode_label} | {family} | pos_enc={pos_enc} | horizon={horizon} ===\")\n",
    "                model, history, test_loss, splits = train_transformer_on_series(\n",
    "                    series,\n",
    "                    input_len=input_len,\n",
    "                    pred_len=horizon,\n",
    "                    n_epochs=n_epochs,\n",
    "                    pos_encoding_type=pos_enc,\n",
    "                )\n",
    "\n",
    "                key = (mode_label, family, pos_enc, horizon)\n",
    "                histories[key] = history\n",
    "                models[key] = model\n",
    "\n",
    "                results.append({\n",
    "                    \"mode\": mode_label,\n",
    "                    \"family\": family,\n",
    "                    \"pos_enc\": pos_enc,\n",
    "                    \"horizon\": horizon,\n",
    "                    \"test_MSE\": float(test_loss),\n",
    "                })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df, histories, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c16932b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== raw | Heavy-tailed AR(1) | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=3.1493 | val=2.9273\n",
      "Epoch 2/10 | train=3.0621 | val=2.8827\n",
      "Epoch 3/10 | train=3.0233 | val=2.8156\n",
      "Epoch 4/10 | train=2.9972 | val=2.8169\n",
      "Epoch 5/10 | train=2.9865 | val=2.8427\n",
      "Epoch 6/10 | train=2.9709 | val=2.8099\n",
      "Epoch 7/10 | train=2.9779 | val=2.7903\n",
      "Epoch 8/10 | train=2.9618 | val=2.7563\n",
      "Epoch 9/10 | train=2.9570 | val=2.7918\n",
      "Epoch 10/10 | train=2.9557 | val=2.8024\n",
      "Test MSE: 2.5840\n",
      "\n",
      "=== raw | Heavy-tailed AR(1) | pos_enc=learned | horizon=1 ===\n",
      "Epoch 1/10 | train=3.2868 | val=2.9425\n",
      "Epoch 2/10 | train=3.0808 | val=2.8487\n",
      "Epoch 3/10 | train=3.0159 | val=2.8571\n",
      "Epoch 4/10 | train=2.9984 | val=2.8137\n",
      "Epoch 5/10 | train=2.9948 | val=2.8384\n",
      "Epoch 6/10 | train=2.9686 | val=2.8328\n",
      "Epoch 7/10 | train=2.9619 | val=2.8182\n",
      "Epoch 8/10 | train=2.9479 | val=2.8048\n",
      "Epoch 9/10 | train=2.9462 | val=2.8109\n",
      "Epoch 10/10 | train=2.9464 | val=2.8424\n",
      "Test MSE: 2.5665\n",
      "\n",
      "=== raw | Heavy-tailed AR(1) | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=3.6621 | val=3.8306\n",
      "Epoch 2/10 | train=3.5971 | val=3.8285\n",
      "Epoch 3/10 | train=3.5902 | val=3.8180\n",
      "Epoch 4/10 | train=3.5904 | val=3.8160\n",
      "Epoch 5/10 | train=3.5838 | val=3.8098\n",
      "Epoch 6/10 | train=3.5764 | val=3.8231\n",
      "Epoch 7/10 | train=3.5773 | val=3.8101\n",
      "Epoch 8/10 | train=3.5758 | val=3.8073\n",
      "Epoch 9/10 | train=3.5728 | val=3.8016\n",
      "Epoch 10/10 | train=3.5719 | val=3.7980\n",
      "Test MSE: 3.9879\n",
      "\n",
      "=== raw | Heavy-tailed AR(1) | pos_enc=learned | horizon=10 ===\n",
      "Epoch 1/10 | train=3.6549 | val=3.8298\n",
      "Epoch 2/10 | train=3.5978 | val=3.8392\n",
      "Epoch 3/10 | train=3.5911 | val=3.8371\n",
      "Epoch 4/10 | train=3.5835 | val=3.8217\n",
      "Epoch 5/10 | train=3.5830 | val=3.8081\n",
      "Epoch 6/10 | train=3.5857 | val=3.8193\n",
      "Epoch 7/10 | train=3.5709 | val=3.8197\n",
      "Epoch 8/10 | train=3.5759 | val=3.8134\n",
      "Epoch 9/10 | train=3.5679 | val=3.8368\n",
      "Epoch 10/10 | train=3.5666 | val=3.8157\n",
      "Test MSE: 3.9975\n",
      "\n",
      "=== raw | GARCH(1,1) | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=2.0835 | val=1.9390\n",
      "Epoch 2/10 | train=1.9971 | val=1.9430\n",
      "Epoch 3/10 | train=1.9927 | val=1.9421\n",
      "Epoch 4/10 | train=1.9871 | val=1.9418\n",
      "Epoch 5/10 | train=1.9841 | val=1.9421\n",
      "Epoch 6/10 | train=1.9755 | val=1.9510\n",
      "Epoch 7/10 | train=1.9872 | val=1.9594\n",
      "Epoch 8/10 | train=1.9724 | val=1.9449\n",
      "Epoch 9/10 | train=1.9743 | val=1.9755\n",
      "Epoch 10/10 | train=1.9834 | val=1.9457\n",
      "Test MSE: 1.9492\n",
      "\n",
      "=== raw | GARCH(1,1) | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=2.0231 | val=1.9344\n",
      "Epoch 2/10 | train=1.9969 | val=1.9244\n",
      "Epoch 3/10 | train=1.9951 | val=1.9206\n",
      "Epoch 4/10 | train=1.9915 | val=1.9247\n",
      "Epoch 5/10 | train=1.9926 | val=1.9238\n",
      "Epoch 6/10 | train=1.9917 | val=1.9223\n",
      "Epoch 7/10 | train=1.9884 | val=1.9334\n",
      "Epoch 8/10 | train=1.9890 | val=1.9202\n",
      "Epoch 9/10 | train=1.9878 | val=1.9289\n",
      "Epoch 10/10 | train=1.9903 | val=1.9275\n",
      "Test MSE: 1.9238\n",
      "\n",
      "=== raw | Regime-switching | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=1.6677 | val=1.5141\n",
      "Epoch 2/10 | train=1.5266 | val=1.4012\n",
      "Epoch 3/10 | train=1.4641 | val=1.4152\n",
      "Epoch 4/10 | train=1.4468 | val=1.4481\n",
      "Epoch 5/10 | train=1.4338 | val=1.4048\n",
      "Epoch 6/10 | train=1.4267 | val=1.3737\n",
      "Epoch 7/10 | train=1.4145 | val=1.3733\n",
      "Epoch 8/10 | train=1.4295 | val=1.5106\n",
      "Epoch 9/10 | train=1.4245 | val=1.3870\n",
      "Epoch 10/10 | train=1.4146 | val=1.3678\n",
      "Test MSE: 1.2682\n",
      "\n",
      "=== raw | Regime-switching | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=2.0160 | val=1.8343\n",
      "Epoch 2/10 | train=1.8171 | val=1.8222\n",
      "Epoch 3/10 | train=1.7830 | val=1.7976\n",
      "Epoch 4/10 | train=1.7674 | val=1.7920\n",
      "Epoch 5/10 | train=1.7576 | val=1.7959\n",
      "Epoch 6/10 | train=1.7551 | val=1.7985\n",
      "Epoch 7/10 | train=1.7494 | val=1.7870\n",
      "Epoch 8/10 | train=1.7532 | val=1.8002\n",
      "Epoch 9/10 | train=1.7524 | val=1.7766\n",
      "Epoch 10/10 | train=1.7452 | val=1.7836\n",
      "Test MSE: 1.7560\n",
      "\n",
      "=== raw | 1/f noise | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=0.6929 | val=0.6276\n",
      "Epoch 2/10 | train=0.6253 | val=0.6184\n",
      "Epoch 3/10 | train=0.6259 | val=0.6134\n",
      "Epoch 4/10 | train=0.6192 | val=0.6146\n",
      "Epoch 5/10 | train=0.6139 | val=0.6265\n",
      "Epoch 6/10 | train=0.6121 | val=0.6062\n",
      "Epoch 7/10 | train=0.6028 | val=0.6020\n",
      "Epoch 8/10 | train=0.6014 | val=0.6036\n",
      "Epoch 9/10 | train=0.6012 | val=0.6003\n",
      "Epoch 10/10 | train=0.5994 | val=0.6157\n",
      "Test MSE: 0.6320\n",
      "\n",
      "=== raw | 1/f noise | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=0.9265 | val=0.8512\n",
      "Epoch 2/10 | train=0.8725 | val=0.8295\n",
      "Epoch 3/10 | train=0.8512 | val=0.8134\n",
      "Epoch 4/10 | train=0.8428 | val=0.8088\n",
      "Epoch 5/10 | train=0.8417 | val=0.7988\n",
      "Epoch 6/10 | train=0.8416 | val=0.8142\n",
      "Epoch 7/10 | train=0.8386 | val=0.7940\n",
      "Epoch 8/10 | train=0.8377 | val=0.7955\n",
      "Epoch 9/10 | train=0.8389 | val=0.7974\n",
      "Epoch 10/10 | train=0.8363 | val=0.7935\n",
      "Test MSE: 0.7911\n",
      "\n",
      "=== raw | Season+Trend+Outliers | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=0.8181 | val=0.2192\n",
      "Epoch 2/10 | train=0.1686 | val=0.1679\n",
      "Epoch 3/10 | train=0.1450 | val=0.1612\n",
      "Epoch 4/10 | train=0.1377 | val=0.1992\n",
      "Epoch 5/10 | train=0.1407 | val=0.1533\n",
      "Epoch 6/10 | train=0.1263 | val=0.1495\n",
      "Epoch 7/10 | train=0.1157 | val=0.1509\n",
      "Epoch 8/10 | train=0.1104 | val=0.1409\n",
      "Epoch 9/10 | train=0.1059 | val=0.1368\n",
      "Epoch 10/10 | train=0.1012 | val=0.1405\n",
      "Test MSE: 0.1344\n",
      "\n",
      "=== raw | Season+Trend+Outliers | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=0.8666 | val=0.5931\n",
      "Epoch 2/10 | train=0.5481 | val=0.4452\n",
      "Epoch 3/10 | train=0.4324 | val=0.3480\n",
      "Epoch 4/10 | train=0.2614 | val=0.1535\n",
      "Epoch 5/10 | train=0.1543 | val=0.1158\n",
      "Epoch 6/10 | train=0.1313 | val=0.0966\n",
      "Epoch 7/10 | train=0.1257 | val=0.0923\n",
      "Epoch 8/10 | train=0.1236 | val=0.1004\n",
      "Epoch 9/10 | train=0.1178 | val=0.0973\n",
      "Epoch 10/10 | train=0.1147 | val=0.0901\n",
      "Test MSE: 0.0943\n",
      "\n",
      "=== raw | Random Walk | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=2796.0251 | val=2591.3120\n",
      "Epoch 2/10 | train=2299.5655 | val=2040.8509\n",
      "Epoch 3/10 | train=1736.6381 | val=1476.7155\n",
      "Epoch 4/10 | train=1223.9827 | val=1012.4197\n",
      "Epoch 5/10 | train=804.2734 | val=615.7648\n",
      "Epoch 6/10 | train=482.4032 | val=365.8532\n",
      "Epoch 7/10 | train=292.2100 | val=222.5694\n",
      "Epoch 8/10 | train=179.4726 | val=132.9846\n",
      "Epoch 9/10 | train=114.1496 | val=79.0536\n",
      "Epoch 10/10 | train=79.8507 | val=48.6650\n",
      "Test MSE: 48.3846\n",
      "\n",
      "=== raw | Random Walk | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=2950.1905 | val=2758.4862\n",
      "Epoch 2/10 | train=2434.3583 | val=2148.1166\n",
      "Epoch 3/10 | train=1806.1506 | val=1532.1808\n",
      "Epoch 4/10 | train=1242.2452 | val=999.4056\n",
      "Epoch 5/10 | train=790.1939 | val=616.5700\n",
      "Epoch 6/10 | train=489.4666 | val=380.9247\n",
      "Epoch 7/10 | train=309.3102 | val=253.4955\n",
      "Epoch 8/10 | train=197.9316 | val=153.4771\n",
      "Epoch 9/10 | train=133.9698 | val=98.7314\n",
      "Epoch 10/10 | train=94.4553 | val=68.1313\n",
      "Test MSE: 73.5609\n",
      "\n",
      "=== raw | Jump Diffusion | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=5397.5463 | val=5038.2416\n",
      "Epoch 2/10 | train=4578.8206 | val=4124.1060\n",
      "Epoch 3/10 | train=3602.4088 | val=3106.4686\n",
      "Epoch 4/10 | train=2617.6436 | val=2179.8946\n",
      "Epoch 5/10 | train=1805.6649 | val=1494.4696\n",
      "Epoch 6/10 | train=1186.8858 | val=898.9364\n",
      "Epoch 7/10 | train=712.9049 | val=553.2859\n",
      "Epoch 8/10 | train=463.0635 | val=363.6167\n",
      "Epoch 9/10 | train=311.0299 | val=236.7187\n",
      "Epoch 10/10 | train=219.4043 | val=161.4282\n",
      "Test MSE: 156.3174\n",
      "\n",
      "=== raw | Jump Diffusion | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=5813.5489 | val=5438.6851\n",
      "Epoch 2/10 | train=5030.9619 | val=4480.8625\n",
      "Epoch 3/10 | train=3970.1062 | val=3379.7641\n",
      "Epoch 4/10 | train=2881.2961 | val=2367.0655\n",
      "Epoch 5/10 | train=1961.0876 | val=1548.4185\n",
      "Epoch 6/10 | train=1236.1918 | val=951.8004\n",
      "Epoch 7/10 | train=757.6386 | val=613.9853\n",
      "Epoch 8/10 | train=489.0128 | val=392.3296\n",
      "Epoch 9/10 | train=334.5937 | val=276.9716\n",
      "Epoch 10/10 | train=278.1788 | val=222.1051\n",
      "Test MSE: 215.8699\n",
      "\n",
      "=== raw | Multi-Seasonality | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=0.2467 | val=0.0277\n",
      "Epoch 2/10 | train=0.0349 | val=0.0199\n",
      "Epoch 3/10 | train=0.0285 | val=0.0185\n",
      "Epoch 4/10 | train=0.0251 | val=0.0233\n",
      "Epoch 5/10 | train=0.0230 | val=0.0174\n",
      "Epoch 6/10 | train=0.0223 | val=0.0190\n",
      "Epoch 7/10 | train=0.0223 | val=0.0175\n",
      "Epoch 8/10 | train=0.0204 | val=0.0182\n",
      "Epoch 9/10 | train=0.0193 | val=0.0200\n",
      "Epoch 10/10 | train=0.0186 | val=0.0162\n",
      "Test MSE: 0.0165\n",
      "\n",
      "=== raw | Multi-Seasonality | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=0.2168 | val=0.0229\n",
      "Epoch 2/10 | train=0.0299 | val=0.0169\n",
      "Epoch 3/10 | train=0.0244 | val=0.0224\n",
      "Epoch 4/10 | train=0.0224 | val=0.0155\n",
      "Epoch 5/10 | train=0.0210 | val=0.0149\n",
      "Epoch 6/10 | train=0.0199 | val=0.0145\n",
      "Epoch 7/10 | train=0.0196 | val=0.0144\n",
      "Epoch 8/10 | train=0.0198 | val=0.0143\n",
      "Epoch 9/10 | train=0.0191 | val=0.0141\n",
      "Epoch 10/10 | train=0.0186 | val=0.0149\n",
      "Test MSE: 0.0149\n",
      "\n",
      "=== raw | Trend Breaks | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=110.5220 | val=68.0816\n",
      "Epoch 2/10 | train=47.4742 | val=23.7822\n",
      "Epoch 3/10 | train=16.1613 | val=6.8609\n",
      "Epoch 4/10 | train=5.5682 | val=2.2247\n",
      "Epoch 5/10 | train=2.5212 | val=0.7807\n",
      "Epoch 6/10 | train=1.4209 | val=0.4694\n",
      "Epoch 7/10 | train=0.9465 | val=0.2180\n",
      "Epoch 8/10 | train=0.7414 | val=0.2024\n",
      "Epoch 9/10 | train=0.6282 | val=0.1251\n",
      "Epoch 10/10 | train=0.5914 | val=0.2386\n",
      "Test MSE: 0.2277\n",
      "\n",
      "=== raw | Trend Breaks | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=141.5678 | val=101.7045\n",
      "Epoch 2/10 | train=64.2429 | val=36.9861\n",
      "Epoch 3/10 | train=21.2616 | val=11.6057\n",
      "Epoch 4/10 | train=6.9159 | val=3.7400\n",
      "Epoch 5/10 | train=2.9069 | val=1.7428\n",
      "Epoch 6/10 | train=1.9160 | val=0.9051\n",
      "Epoch 7/10 | train=1.2953 | val=0.5390\n",
      "Epoch 8/10 | train=1.1239 | val=0.9647\n",
      "Epoch 9/10 | train=1.0134 | val=0.3135\n",
      "Epoch 10/10 | train=0.9220 | val=0.2481\n",
      "Test MSE: 0.2773\n",
      "\n",
      "=== fixed | Heavy-tailed AR(1) | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=0.8283 | val=0.7440\n",
      "Epoch 2/10 | train=0.7900 | val=0.7395\n",
      "Epoch 3/10 | train=0.7887 | val=0.7377\n",
      "Epoch 4/10 | train=0.7818 | val=0.7371\n",
      "Epoch 5/10 | train=0.7842 | val=0.7278\n",
      "Epoch 6/10 | train=0.7827 | val=0.7355\n",
      "Epoch 7/10 | train=0.7785 | val=0.7402\n",
      "Epoch 8/10 | train=0.7835 | val=0.7286\n",
      "Epoch 9/10 | train=0.7785 | val=0.7352\n",
      "Epoch 10/10 | train=0.7797 | val=0.7334\n",
      "Test MSE: 0.6700\n",
      "\n",
      "=== fixed | Heavy-tailed AR(1) | pos_enc=learned | horizon=1 ===\n",
      "Epoch 1/10 | train=0.8328 | val=0.7485\n",
      "Epoch 2/10 | train=0.7961 | val=0.7365\n",
      "Epoch 3/10 | train=0.7927 | val=0.7410\n",
      "Epoch 4/10 | train=0.7816 | val=0.7294\n",
      "Epoch 5/10 | train=0.7853 | val=0.7631\n",
      "Epoch 6/10 | train=0.7821 | val=0.7334\n",
      "Epoch 7/10 | train=0.7826 | val=0.7276\n",
      "Epoch 8/10 | train=0.7756 | val=0.7308\n",
      "Epoch 9/10 | train=0.7744 | val=0.7295\n",
      "Epoch 10/10 | train=0.7701 | val=0.7371\n",
      "Test MSE: 0.6759\n",
      "\n",
      "=== fixed | Heavy-tailed AR(1) | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=0.9792 | val=1.0107\n",
      "Epoch 2/10 | train=0.9562 | val=1.0097\n",
      "Epoch 3/10 | train=0.9520 | val=1.0075\n",
      "Epoch 4/10 | train=0.9491 | val=1.0130\n",
      "Epoch 5/10 | train=0.9489 | val=1.0100\n",
      "Epoch 6/10 | train=0.9468 | val=1.0100\n",
      "Epoch 7/10 | train=0.9485 | val=1.0068\n",
      "Epoch 8/10 | train=0.9469 | val=1.0044\n",
      "Epoch 9/10 | train=0.9465 | val=1.0057\n",
      "Epoch 10/10 | train=0.9453 | val=1.0098\n",
      "Test MSE: 1.0586\n",
      "\n",
      "=== fixed | Heavy-tailed AR(1) | pos_enc=learned | horizon=10 ===\n",
      "Epoch 1/10 | train=0.9871 | val=1.0117\n",
      "Epoch 2/10 | train=0.9546 | val=1.0130\n",
      "Epoch 3/10 | train=0.9525 | val=1.0129\n",
      "Epoch 4/10 | train=0.9507 | val=1.0077\n",
      "Epoch 5/10 | train=0.9462 | val=1.0055\n",
      "Epoch 6/10 | train=0.9463 | val=1.0070\n",
      "Epoch 7/10 | train=0.9461 | val=1.0099\n",
      "Epoch 8/10 | train=0.9460 | val=1.0096\n",
      "Epoch 9/10 | train=0.9440 | val=1.0060\n",
      "Epoch 10/10 | train=0.9430 | val=1.0045\n",
      "Test MSE: 1.0534\n",
      "\n",
      "=== fixed | GARCH(1,1) | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=1.0224 | val=0.9855\n",
      "Epoch 2/10 | train=1.0061 | val=0.9928\n",
      "Epoch 3/10 | train=1.0044 | val=0.9835\n",
      "Epoch 4/10 | train=1.0044 | val=0.9873\n",
      "Epoch 5/10 | train=1.0085 | val=0.9809\n",
      "Epoch 6/10 | train=0.9982 | val=0.9819\n",
      "Epoch 7/10 | train=0.9991 | val=0.9826\n",
      "Epoch 8/10 | train=1.0042 | val=0.9813\n",
      "Epoch 9/10 | train=1.0018 | val=0.9928\n",
      "Epoch 10/10 | train=1.0003 | val=0.9859\n",
      "Test MSE: 0.9914\n",
      "\n",
      "=== fixed | GARCH(1,1) | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=1.0209 | val=0.9777\n",
      "Epoch 2/10 | train=1.0113 | val=0.9807\n",
      "Epoch 3/10 | train=1.0094 | val=0.9726\n",
      "Epoch 4/10 | train=1.0081 | val=0.9711\n",
      "Epoch 5/10 | train=1.0074 | val=0.9823\n",
      "Epoch 6/10 | train=1.0073 | val=0.9734\n",
      "Epoch 7/10 | train=1.0060 | val=0.9725\n",
      "Epoch 8/10 | train=1.0056 | val=0.9711\n",
      "Epoch 9/10 | train=1.0055 | val=0.9720\n",
      "Epoch 10/10 | train=1.0047 | val=0.9742\n",
      "Test MSE: 0.9734\n",
      "\n",
      "=== fixed | Regime-switching | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=0.8285 | val=0.7096\n",
      "Epoch 2/10 | train=0.8070 | val=0.7176\n",
      "Epoch 3/10 | train=0.8023 | val=0.7197\n",
      "Epoch 4/10 | train=0.7991 | val=0.7160\n",
      "Epoch 5/10 | train=0.8027 | val=0.7115\n",
      "Epoch 6/10 | train=0.7946 | val=0.7132\n",
      "Epoch 7/10 | train=0.7869 | val=0.6926\n",
      "Epoch 8/10 | train=0.7597 | val=0.6734\n",
      "Epoch 9/10 | train=0.7538 | val=0.6547\n",
      "Epoch 10/10 | train=0.7415 | val=0.6677\n",
      "Test MSE: 0.7891\n",
      "\n",
      "=== fixed | Regime-switching | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=1.0093 | val=1.0165\n",
      "Epoch 2/10 | train=0.9852 | val=1.0170\n",
      "Epoch 3/10 | train=0.9810 | val=1.0170\n",
      "Epoch 4/10 | train=0.9792 | val=1.0184\n",
      "Epoch 5/10 | train=0.9781 | val=1.0188\n",
      "Epoch 6/10 | train=0.9745 | val=1.0134\n",
      "Epoch 7/10 | train=0.9726 | val=1.0101\n",
      "Epoch 8/10 | train=0.9714 | val=1.0085\n",
      "Epoch 9/10 | train=0.9705 | val=1.0118\n",
      "Epoch 10/10 | train=0.9722 | val=1.0093\n",
      "Test MSE: 0.9624\n",
      "\n",
      "=== fixed | 1/f noise | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=0.6764 | val=0.6419\n",
      "Epoch 2/10 | train=0.6272 | val=0.6400\n",
      "Epoch 3/10 | train=0.6273 | val=0.6216\n",
      "Epoch 4/10 | train=0.6140 | val=0.6214\n",
      "Epoch 5/10 | train=0.6134 | val=0.6212\n",
      "Epoch 6/10 | train=0.6096 | val=0.6105\n",
      "Epoch 7/10 | train=0.6037 | val=0.6198\n",
      "Epoch 8/10 | train=0.6024 | val=0.6057\n",
      "Epoch 9/10 | train=0.6074 | val=0.6185\n",
      "Epoch 10/10 | train=0.6039 | val=0.6197\n",
      "Test MSE: 0.6413\n",
      "\n",
      "=== fixed | 1/f noise | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=0.9059 | val=0.8357\n",
      "Epoch 2/10 | train=0.8593 | val=0.8070\n",
      "Epoch 3/10 | train=0.8466 | val=0.7947\n",
      "Epoch 4/10 | train=0.8401 | val=0.7901\n",
      "Epoch 5/10 | train=0.8350 | val=0.8052\n",
      "Epoch 6/10 | train=0.8374 | val=0.7894\n",
      "Epoch 7/10 | train=0.8372 | val=0.7926\n",
      "Epoch 8/10 | train=0.8329 | val=0.7948\n",
      "Epoch 9/10 | train=0.8336 | val=0.7900\n",
      "Epoch 10/10 | train=0.8279 | val=0.7895\n",
      "Test MSE: 0.7904\n",
      "\n",
      "=== fixed | Season+Trend+Outliers | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=0.8576 | val=1.9846\n",
      "Epoch 2/10 | train=0.7862 | val=1.9596\n",
      "Epoch 3/10 | train=0.7517 | val=1.9291\n",
      "Epoch 4/10 | train=0.7263 | val=1.9143\n",
      "Epoch 5/10 | train=0.7175 | val=1.8446\n",
      "Epoch 6/10 | train=0.6910 | val=1.8313\n",
      "Epoch 7/10 | train=0.6807 | val=1.8438\n",
      "Epoch 8/10 | train=0.6802 | val=1.8602\n",
      "Epoch 9/10 | train=0.6843 | val=1.8365\n",
      "Epoch 10/10 | train=0.6639 | val=1.8447\n",
      "Test MSE: 0.2216\n",
      "\n",
      "=== fixed | Season+Trend+Outliers | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=1.0267 | val=0.9778\n",
      "Epoch 2/10 | train=0.9535 | val=0.9714\n",
      "Epoch 3/10 | train=0.9488 | val=0.9626\n",
      "Epoch 4/10 | train=0.9435 | val=0.9740\n",
      "Epoch 5/10 | train=0.9445 | val=0.9733\n",
      "Epoch 6/10 | train=0.9336 | val=0.9643\n",
      "Epoch 7/10 | train=0.9387 | val=0.9734\n",
      "Epoch 8/10 | train=0.9359 | val=0.9645\n",
      "Epoch 9/10 | train=0.9337 | val=0.9601\n",
      "Epoch 10/10 | train=0.9321 | val=0.9661\n",
      "Test MSE: 1.0578\n",
      "\n",
      "=== fixed | Random Walk | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=1.0405 | val=0.9257\n",
      "Epoch 2/10 | train=1.0373 | val=0.9375\n",
      "Epoch 3/10 | train=1.0260 | val=0.9398\n",
      "Epoch 4/10 | train=1.0297 | val=0.9280\n",
      "Epoch 5/10 | train=1.0376 | val=0.9287\n",
      "Epoch 6/10 | train=1.0273 | val=0.9348\n",
      "Epoch 7/10 | train=1.0259 | val=0.9266\n",
      "Epoch 8/10 | train=1.0274 | val=0.9387\n",
      "Epoch 9/10 | train=1.0255 | val=0.9262\n",
      "Epoch 10/10 | train=1.0267 | val=0.9273\n",
      "Test MSE: 0.9861\n",
      "\n",
      "=== fixed | Random Walk | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=1.0297 | val=1.0253\n",
      "Epoch 2/10 | train=1.0111 | val=1.0258\n",
      "Epoch 3/10 | train=1.0100 | val=1.0220\n",
      "Epoch 4/10 | train=1.0085 | val=1.0215\n",
      "Epoch 5/10 | train=1.0088 | val=1.0223\n",
      "Epoch 6/10 | train=1.0074 | val=1.0202\n",
      "Epoch 7/10 | train=1.0055 | val=1.0190\n",
      "Epoch 8/10 | train=1.0058 | val=1.0184\n",
      "Epoch 9/10 | train=1.0057 | val=1.0196\n",
      "Epoch 10/10 | train=1.0041 | val=1.0174\n",
      "Test MSE: 0.9996\n",
      "\n",
      "=== fixed | Jump Diffusion | pos_enc=sin | horizon=1 ===\n",
      "Epoch 1/10 | train=0.0483 | val=0.0037\n",
      "Epoch 2/10 | train=0.0080 | val=0.0021\n",
      "Epoch 3/10 | train=0.0053 | val=0.0026\n",
      "Epoch 4/10 | train=0.0044 | val=0.0040\n",
      "Epoch 5/10 | train=0.0040 | val=0.0026\n",
      "Epoch 6/10 | train=0.0037 | val=0.0018\n",
      "Epoch 7/10 | train=0.0034 | val=0.0018\n",
      "Epoch 8/10 | train=0.0033 | val=0.0027\n",
      "Epoch 9/10 | train=0.0036 | val=0.0018\n",
      "Epoch 10/10 | train=0.0031 | val=0.0022\n",
      "Test MSE: 0.0027\n",
      "\n",
      "=== fixed | Jump Diffusion | pos_enc=sin | horizon=10 ===\n",
      "Epoch 1/10 | train=0.1776 | val=0.0201\n",
      "Epoch 2/10 | train=0.0236 | val=0.0139\n",
      "Epoch 3/10 | train=0.0171 | val=0.0138\n",
      "Epoch 4/10 | train=0.0152 | val=0.0125\n",
      "Epoch 5/10 | train=0.0139 | val=0.0123\n",
      "Epoch 6/10 | train=0.0133 | val=0.0119\n",
      "Epoch 7/10 | train=0.0129 | val=0.0122\n",
      "Epoch 8/10 | train=0.0127 | val=0.0116\n",
      "Epoch 9/10 | train=0.0124 | val=0.0127\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m raw_results_df, raw_histories, raw_models \u001b[38;5;241m=\u001b[39m run_experiment_suite(\n\u001b[1;32m      3\u001b[0m     raw_dict,\n\u001b[1;32m      4\u001b[0m     mode_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     horizons\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m      6\u001b[0m     n_epochs\u001b[38;5;241m=\u001b[39mn_epochs,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run fixed (stationary) series experiments\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m fixed_results_df, fixed_histories, fixed_models \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment_suite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfixed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhorizons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Combine results\u001b[39;00m\n\u001b[1;32m     18\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([raw_results_df, fixed_results_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[22], line 37\u001b[0m, in \u001b[0;36mrun_experiment_suite\u001b[0;34m(series_dict, mode_label, horizons, n_epochs, preprocessing_fn)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos_enc \u001b[38;5;129;01min\u001b[39;00m pos_encs:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfamily\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | pos_enc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_enc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | horizon=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhorizon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m     model, history, test_loss, splits \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_transformer_on_series\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhorizon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_encoding_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_enc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     key \u001b[38;5;241m=\u001b[39m (mode_label, family, pos_enc, horizon)\n\u001b[1;32m     46\u001b[0m     histories[key] \u001b[38;5;241m=\u001b[39m history\n",
      "Cell \u001b[0;32mIn[18], line 73\u001b[0m, in \u001b[0;36mtrain_transformer_on_series\u001b[0;34m(series, input_len, pred_len, batch_size, n_epochs, lr, pos_encoding_type, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     72\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 73\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_batch)\n\u001b[1;32m     75\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 148\u001b[0m, in \u001b[0;36mTimeSeriesTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(h)\n\u001b[0;32m--> 148\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# (B, L, d_model)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m h_last \u001b[38;5;241m=\u001b[39m h[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]    \u001b[38;5;66;03m# (B, d_model)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(h_last)                     \u001b[38;5;66;03m# (B, pred_len * input_dim)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:415\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    412\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 415\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    418\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 134\u001b[0m, in \u001b[0;36mTimeSeriesTransformer.__init__.<locals>.layer_forward\u001b[0;34m(src, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlayer_forward\u001b[39m(src, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcustom_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msrc_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msrc_key_padding_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 123\u001b[0m, in \u001b[0;36mTimeSeriesTransformer.__init__.<locals>.custom_forward\u001b[0;34m(module, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_forward\u001b[39m(module, src, src_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, src_key_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 123\u001b[0m     attn_output, attn_weight \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     module\u001b[38;5;241m.\u001b[39mattn_weight \u001b[38;5;241m=\u001b[39m attn_weight  \u001b[38;5;66;03m# save for extraction\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     src \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mnorm1(src \u001b[38;5;241m+\u001b[39m module\u001b[38;5;241m.\u001b[39mdropout1(attn_output))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/activation.py:1266\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1253\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1263\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1264\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1266\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:5470\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5469\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m-> 5470\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   5472\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m dropout(attn_output_weights, p\u001b[38;5;241m=\u001b[39mdropout_p)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:1885\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1883\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1887\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run raw series experiments\n",
    "raw_results_df, raw_histories, raw_models = run_experiment_suite(\n",
    "    raw_dict,\n",
    "    mode_label=\"raw\",\n",
    "    horizons=(1, 10),\n",
    "    n_epochs=n_epochs,\n",
    ")\n",
    "\n",
    "# Run fixed (stationary) series experiments\n",
    "fixed_results_df, fixed_histories, fixed_models = run_experiment_suite(\n",
    "    fixed_dict,\n",
    "    mode_label=\"fixed\",\n",
    "    horizons=(1, 10),\n",
    "    n_epochs=n_epochs,\n",
    ")\n",
    "\n",
    "# Combine results\n",
    "combined_df = pd.concat([raw_results_df, fixed_results_df], ignore_index=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b596707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualizations\n",
    "def plot_mse_raw_vs_stationary(combined_df):\n",
    "    for horizon in sorted(combined_df[\"horizon\"].unique()):\n",
    "        df_h = combined_df[combined_df[\"horizon\"] == horizon]\n",
    "\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        # pivot: index=family+pos_enc, columns=mode, values=test_MSE\n",
    "        df_pivot = df_h.pivot_table(\n",
    "            index=[\"family\", \"pos_enc\"],\n",
    "            columns=\"mode\",\n",
    "            values=\"test_MSE\",\n",
    "        )\n",
    "\n",
    "        df_pivot.plot(kind=\"bar\", ax=plt.gca())\n",
    "        plt.title(f\"Test MSE – horizon={horizon} (raw vs stationary)\")\n",
    "        plt.ylabel(\"Test MSE\")\n",
    "        plt.xlabel(\"Family / positional encoding\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_mse_raw_vs_stationary(combined_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
